{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a7c1d19",
   "metadata": {},
   "source": [
    "# Exercice 1 :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3fa066",
   "metadata": {},
   "source": [
    "###  1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a4df9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hello_world_mpi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hello_world_mpi.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "RANK = COMM.Get_rank()\n",
    "SIZE = COMM.Get_size()\n",
    "\n",
    "print(\"Hello world !\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73428afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world !\r\n",
      "Hello world !\r\n",
      "Hello world !\r\n",
      "Hello world !\r\n"
     ]
    }
   ],
   "source": [
    "!mpirun -n 4 python3  hello_world_mpi.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3458ec6c",
   "metadata": {},
   "source": [
    "### 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "549ebbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hello_world_mpi_r_s.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hello_world_mpi_r_s.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "RANK = COMM.Get_rank()\n",
    "SIZE = COMM.Get_size()\n",
    "\n",
    "def h_w():\n",
    "    print(\"Hello world !\",RANK,SIZE)\n",
    "    \n",
    "h_w()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c2dc8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world !\r\n",
      "Hello world !\r\n",
      "Hello world !\r\n",
      "Hello world !\r\n",
      "Hello world !\r\n",
      "Hello world !\r\n",
      "Hello world !\r\n",
      "Hello world !\r\n"
     ]
    }
   ],
   "source": [
    "!mpirun --oversubscribe -n 8 python3  hello_world_mpi.py #to use more than the real cores existing on the machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d820d9d9",
   "metadata": {},
   "source": [
    "### 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b716a9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hello_world_mpi_r_s_0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hello_world_mpi_r_s_0.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "RANK = COMM.Get_rank()\n",
    "SIZE = COMM.Get_size()\n",
    "\n",
    "if RANK==0 :\n",
    "    print(\"Hello world !\",RANK,SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f8e8366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world !\r\n",
      "Hello world !\r\n",
      "Hello world !\r\n",
      "Hello world !\r\n"
     ]
    }
   ],
   "source": [
    "!mpirun -n 4 python3  hello_world_mpi.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8252f717",
   "metadata": {},
   "source": [
    "# Exercice 2 :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ba9b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile neg_value_exit.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "# Initialize MPI environment\n",
    "COMM = MPI.COMM_WORLD\n",
    "\n",
    "# Get the rank of the current process\n",
    "RANK= COMM.Get_rank()\n",
    "\n",
    "while True:\n",
    "    if RANK == 0:\n",
    "        # Only the controller process (rank 0) reads the input\n",
    "        value = int(input(\"Enter an integer value (or a negative integer to exit):\"))\n",
    "        \n",
    "    else:\n",
    "        value = None\n",
    "        \n",
    "    # Send the value to all processes\n",
    "    value = COMM.bcast(value, root=0)\n",
    "    \n",
    "    # Exit the program in case of negative input\n",
    "    if value < 0:\n",
    "        break\n",
    "\n",
    "    \n",
    "    # Print the received value and the rank of the current process\n",
    "    print(f\"Process {RANK} got {value}\")\n",
    "    COMM.Barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be64f639",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -n 4 python3  neg_value_exit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0199c75e",
   "metadata": {},
   "source": [
    "# Exercice 3 :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7655ecc",
   "metadata": {},
   "source": [
    "### 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7f133b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile broadcast_by_ring.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "# Initialize MPI environment\n",
    "COMM = MPI.COMM_WORLD\n",
    "\n",
    "# Get the rank of the current process\n",
    "RANK= COMM.Get_rank()\n",
    "\n",
    "# Get the number of the machine cores\n",
    "SIZE = COMM.Get_size()\n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "    if RANK == 0:\n",
    "        \n",
    "        #Only the controller process (rank 0) reads the input\n",
    "        value = int(input(\"Enter an integer value (or a negative integer to exit):\"))\n",
    "        \n",
    "        COMM.send( value, dest= 1)\n",
    "        \n",
    "    \n",
    "    else :\n",
    "        \n",
    "        value = COMM.recv( source= RANK-1)\n",
    "        \n",
    "        if RANK < SIZE-1 :\n",
    "            COMM.send( value + RANK, dest= RANK + 1)\n",
    "        \n",
    "        \n",
    "    if value < 0 :\n",
    "        \n",
    "        break\n",
    "        \n",
    "   \n",
    "        \n",
    "    # Print the received value and the rank of the current process\n",
    "    \n",
    "    print(f\"Process {RANK} got {value}\")\n",
    "    COMM.Barrier()\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66708ef6",
   "metadata": {},
   "source": [
    "# Exercice 4 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0dee1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n",
      "[[ 0.  1.  2.  3.]\n",
      " [ 4.  5.  6.  7.]\n",
      " [ 8.  9. 10. 11.]\n",
      " [12. 13. 14. 15.]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 23\u001b[0m\n\u001b[1;32m     16\u001b[0m     matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#Question 2 :\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# divide the matrix into the required submatrixes parts and scatter to the other processes\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#submatrix of processor 0\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m sub_matrix_0 \u001b[38;5;241m=\u001b[39m \u001b[43mmatrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#submatrix of processor 1\u001b[39;00m\n\u001b[1;32m     26\u001b[0m sub_matrix_1 \u001b[38;5;241m=\u001b[39m matrix[ \u001b[38;5;241m0\u001b[39m : (n\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m , m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m : m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m ]\n",
      "\u001b[0;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "COMM = MPI.COMM_WORLD\n",
    "RANK = COMM.Get_rank()\n",
    "SIZE = COMM.Get_size()\n",
    "\n",
    "#Question 1 :\n",
    "\n",
    "n, m = map(int, input().split())\n",
    "\n",
    "# create the matrix on process 0\n",
    "if RANK == 0:\n",
    "    matrix = np.arange(n*m, dtype='float64').reshape(n, m)\n",
    "    print(matrix)\n",
    "else:\n",
    "    matrix = None\n",
    "    \n",
    "#Question 2 :\n",
    "\n",
    "# divide the matrix into the required submatrixes parts and scatter to the other processes\n",
    "\n",
    "#submatrix of processor 0\n",
    "sub_matrix_0 = matrix[ 0 : (n/2)-1 , 0 : (m/2)-1 ]\n",
    "\n",
    "#submatrix of processor 1\n",
    "sub_matrix_1 = matrix[ 0 : (n/2)-1 , m/2 : m-1 ]\n",
    "\n",
    "#submatrix of processor 2\n",
    "sub_matrix_2 = matrix[ n/2 : n-1 , 0 : (m/2)-1 ]\n",
    "\n",
    "#submatrix of processor 3\n",
    "sub_matrix_3 = matrix[ n/2 : n-1 , m/2 : m-1 ]\n",
    "\n",
    "#constructing array of submatrix to help the scatter function\n",
    "\n",
    "subm_arr = np.array([sub_matrix_0,sub_matrix_1,sub_matrix_2,sub_matrix_3])\n",
    "\n",
    "data = COMM.scatterv(subm_arr, root=0)\n",
    "\n",
    "''' \n",
    "# Scatter the matrix parts to each processor\n",
    "recvA = np.zeros((n // 2, m // 2))\n",
    "recvcounts = (n // 2) * (m // 2)\n",
    "print(np.transpose(A))\n",
    "comm.Scatterv([np.transpose(A) , sendcounts, displs, MPI.DOUBLE], recvA, root=0)\n",
    "if rank == 1:\n",
    "    print(\"Received matrix on processor 1:\")\n",
    "    print(recvA)\n",
    "elif rank == 2:\n",
    "    print(\"Received matrix on processor 2:\")\n",
    "    print(recvA)\n",
    "elif rank == 3:\n",
    "    print(\"Received matrix on processor 2:\")\n",
    "    print(recvA)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "322ef7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      " [[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]\n",
      " [16 17 18 19]\n",
      " [20 21 22 23]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'int' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Define scatterv parameters\u001b[39;00m\n\u001b[1;32m     21\u001b[0m counts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(n\u001b[38;5;241m*\u001b[39mm\u001b[38;5;241m/\u001b[39msize)] \u001b[38;5;241m*\u001b[39m size\n\u001b[0;32m---> 22\u001b[0m counts[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mn\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     23\u001b[0m displs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(counts[:i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(size)]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Scatter parts of matrix A to other processors\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'int' and 'list'"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# Define matrix size\n",
    "n = 6\n",
    "m = 4\n",
    "\n",
    "# Create matrix A on processor 0\n",
    "if rank == 0:\n",
    "    A = np.arange(n*m).reshape((n, m))\n",
    "    print(\"Matrix A:\\n\", A)\n",
    "else:\n",
    "    A = None\n",
    "\n",
    "# Define scatterv parameters\n",
    "counts = [int(n*m/size)] * size\n",
    "counts[0] = n*m - counts[1:]\n",
    "displs = [sum(counts[:i]) for i in range(size)]\n",
    "\n",
    "# Scatter parts of matrix A to other processors\n",
    "recvbuf = np.zeros(int(n*m/size), dtype='i')\n",
    "comm.Scatterv([A, counts, displs, MPI.INT], recvbuf, root=0)\n",
    "\n",
    "# Processor 1 receives A(i,j) for i=0 to (n/2)-1 and j=m/2 to m-1\n",
    "if rank == 1:\n",
    "    recvbuf = recvbuf.reshape((int(n/2), int(m/2)))\n",
    "    print(\"Processor\", rank, \"received:\\n\", recvbuf)\n",
    "\n",
    "# Processor 2 receives A(i,j) for i=n/2 to n-1 and j=0 to (m/2)-1\n",
    "if rank == 2:\n",
    "    recvbuf = recvbuf.reshape((int(n/2), int(m/2)))\n",
    "    print(\"Processor\", rank, \"received:\\n\", recvbuf)\n",
    "\n",
    "# Processor 3 receives A(i,j) for i=n/2 to n-1 and j=m/2 to m-1\n",
    "if rank == 3:\n",
    "    recvbuf = recvbuf.reshape((int(n/2), int(m/2)))\n",
    "    print(\"Processor\", rank, \"received:\\n\", recvbuf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5dff44",
   "metadata": {},
   "source": [
    "# Exercice 5 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e064d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.sparse import lil_matrix\n",
    "from numpy.random import rand, seed\n",
    "from numba import njit\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "''' This program compute parallel csc matrix vector multiplication using mpi '''\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "size = COMM.Get_size()\n",
    "RANK = COMM.Get_rank()\n",
    "\n",
    "seed(42)\n",
    "\n",
    "def matrixVectorMult(A, b, x, size):\n",
    "    \n",
    "    row, col = A.shape\n",
    "    start = RANK * (row // size)\n",
    "    end = start + (row // size)\n",
    "   #x = np.zeros(col)\n",
    "    for i in range(start, end):\n",
    "       # a = A[i]\n",
    "        for j in range(col):\n",
    "            x[i] += A[i,j] * b[j]\n",
    "\n",
    "    return x\n",
    "\n",
    "########################initialize matrix A and vector b ######################\n",
    "#matrix sizes\n",
    "SIZE = 1000\n",
    "#Local_size = \n",
    "\n",
    "# counts = block of each proc\n",
    "#counts = \n",
    "\n",
    "if RANK == 0:\n",
    "    A = lil_matrix((SIZE, SIZE))\n",
    "    A[0, :100] = rand(100)\n",
    "    A[1, 100:200] = A[0, :100]\n",
    "    A.setdiag(rand(SIZE))\n",
    "    A = A.toarray()\n",
    "    b = rand(SIZE)\n",
    "else :\n",
    "    A = None\n",
    "    b = None\n",
    "\n",
    "\n",
    "#########Send b to all procs and scatter A (each proc has its own local matrix#####\n",
    "LocalMatrix = lil_matrix((SIZE, SIZE))\n",
    "# Scatter the matrix A\n",
    "#COMM.Scatter(A, LocalMatrix, root=0)\n",
    "#####################Compute A*b locally#######################################\n",
    "b = np.zeros(SIZE)\n",
    "Localb = np.zeros(SIZE) \n",
    "#LocalX = np.zeros(SIZE)\n",
    "#COMM.Scatter(b, Localb, root=0)\n",
    "\n",
    "LocalX = np.zeros(SIZE)\n",
    "LocalX = matrixVectorMult(LocalMatrix, b, LocalX, size)\n",
    "if RANK == 0:\n",
    "    X = np.zeros(SIZE)\n",
    "\n",
    "#recvcounts = np.full(4, SIZE // 4)\n",
    "#displs = np.arange(4) * (SIZE // 4)\n",
    "#COMM.Gatherv(LocalX, [X, recvcounts, displs, MPI.DOUBLE], root = 0)\n",
    "# start = MPI.Wtime()\n",
    "# matrixVectorMult(LocalMatrix, b, LocalX)\n",
    "# stop = MPI.Wtime()\n",
    "\n",
    "if RANK == 0:\n",
    "    start = MPI.Wtime()\n",
    "    matrixVectorMult(LocalMatrix, b, LocalX, size)\n",
    "    stop = MPI.Wtime()\n",
    "    print(\"CPU time of parallel multiplication is \", (stop - start))\n",
    "\n",
    "##################Gather te results ###########################################\n",
    "# sendcouns = local size of result\n",
    "#sendcounts = \n",
    "if RANK == 0:\n",
    "    X = matrixVectorMult(A, b, LocalX, size) \n",
    "else :\n",
    "    X = None\n",
    "\n",
    "# Gather the result into X\n",
    "\n",
    "\n",
    "##################Print the results ###########################################\n",
    "\n",
    "if RANK == 0 :\n",
    "    start = MPI.Wtime()\n",
    "    X_ = A.dot(b)\n",
    "    stop = MPI.Wtime()\n",
    "    print(\"The time to calculate A*b using dot is :\", stop - start)\n",
    "    # print(\"The result of A*b using parallel version is :\", X)\n",
    "    \n",
    "    \n",
    "\n",
    "if RANK == 0:\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(X)\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Parallel Matrix-Vector Multiplication')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682f0d07",
   "metadata": {},
   "source": [
    "# Exercice 6 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0764c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from mpi4py import MPI\n",
    "import math\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "N = 840\n",
    "\n",
    "pi_part = 0.0\n",
    "for i in range(rank, N, size):\n",
    "    pi_part += 1.0/(1.0 + ((i + 0.5)/N)**2)\n",
    "pi_part *= 4.0/N\n",
    "#Print the computed value of pi for each process\n",
    "print(\"Process\", rank, \"partial sum:\", pi_part)\n",
    "if rank == 0:\n",
    "    pi = pi_part\n",
    "    for i in range(1, size):\n",
    "        pi += comm.recv(source=i)\n",
    "else:\n",
    "    comm.send(pi_part, dest=0)\n",
    "if rank == 0:\n",
    "    print(\"Computed pi:\", pi)\n",
    "comm.Barrier() \n",
    "t1 = MPI.Wtime()\n",
    "num_runs = 10000 \n",
    "for i in range(num_runs):\n",
    "    pi_part = 0.0\n",
    "    for j in range(rank, N, size):\n",
    "        #Divide the iterations among the processes\n",
    "        pi_part += 1.0/(1.0 + ((j + 0.5)/N)**2)\n",
    "    pi_part *= 4.0/N\n",
    "    #Collect and sum the partial results on the rank 0 process\n",
    "    if rank == 0:\n",
    "        pi = pi_part\n",
    "        for j in range(1, size):\n",
    "            pi += comm.recv(source=j)\n",
    "    else:\n",
    "        comm.send(pi_part, dest=0)\n",
    "    if rank == 0 and i == 0:\n",
    "        print(\"Computed pi:\", pi)\n",
    "comm.Barrier()\n",
    "t2 = MPI.Wtime()\n",
    "if rank == 0:\n",
    "    print(\"Taken Time:\", t2 - t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
