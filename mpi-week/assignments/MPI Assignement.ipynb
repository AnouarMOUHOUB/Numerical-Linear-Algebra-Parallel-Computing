{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a7c1d19",
   "metadata": {},
   "source": [
    "# Exercice 1 :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3fa066",
   "metadata": {},
   "source": [
    "###  1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a4df9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hello_world_mpi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hello_world_mpi.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "RANK = COMM.Get_rank()\n",
    "SIZE = COMM.Get_size()\n",
    "\n",
    "print(\"Hello world !\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73428afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world !\r\n",
      "Hello world !\r\n",
      "Hello world !\r\n",
      "Hello world !\r\n"
     ]
    }
   ],
   "source": [
    "!mpirun -n 4 python3  hello_world_mpi.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3458ec6c",
   "metadata": {},
   "source": [
    "### 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "549ebbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hello_world_mpi_r_s.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hello_world_mpi_r_s.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "RANK = COMM.Get_rank()\n",
    "SIZE = COMM.Get_size()\n",
    "\n",
    "def h_w():\n",
    "    print(\"Hello world !\",RANK,SIZE)\n",
    "    \n",
    "h_w()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c2dc8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world !\n",
      "Hello world !\n",
      "Hello world !\n",
      "Hello world !\n",
      "Hello world !\n",
      "Hello world !\n",
      "Hello world !\n",
      "Hello world !\n"
     ]
    }
   ],
   "source": [
    "!mpirun --oversubscribe -n 8 python3  hello_world_mpi.py #to use more than the real cores existing on the machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d820d9d9",
   "metadata": {},
   "source": [
    "### 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b716a9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hello_world_mpi_r_s_0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hello_world_mpi_r_s_0.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "RANK = COMM.Get_rank()\n",
    "SIZE = COMM.Get_size()\n",
    "\n",
    "if RANK==0 :\n",
    "    print(\"Hello world !\",RANK,SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f8e8366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world !\r\n",
      "Hello world !\r\n",
      "Hello world !\r\n",
      "Hello world !\r\n"
     ]
    }
   ],
   "source": [
    "!mpirun -n 4 python3  hello_world_mpi.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8252f717",
   "metadata": {},
   "source": [
    "# Exercice 2 :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ba9b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile neg_value_exit.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "# Initialize MPI environment\n",
    "COMM = MPI.COMM_WORLD\n",
    "\n",
    "# Get the rank of the current process\n",
    "RANK= COMM.Get_rank()\n",
    "\n",
    "while True:\n",
    "    if RANK == 0:\n",
    "        # Only the controller process (rank 0) reads the input\n",
    "        value = int(input(\"Enter an integer value (or a negative integer to exit):\"))\n",
    "        \n",
    "    else:\n",
    "        value = None\n",
    "        \n",
    "    # Send the value to all processes\n",
    "    value = COMM.bcast(value, root=0)\n",
    "    \n",
    "    # Exit the program in case of negative input\n",
    "    if value < 0:\n",
    "        break\n",
    "\n",
    "    \n",
    "    # Print the received value and the rank of the current process\n",
    "    print(f\"Process {RANK} got {value}\")\n",
    "    COMM.Barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be64f639",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -n 4 python3  neg_value_exit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0199c75e",
   "metadata": {},
   "source": [
    "# Exercice 3 :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7655ecc",
   "metadata": {},
   "source": [
    "### 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7f133b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile broadcast_by_ring.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "# Initialize MPI environment\n",
    "COMM = MPI.COMM_WORLD\n",
    "\n",
    "# Get the rank of the current process\n",
    "RANK= COMM.Get_rank()\n",
    "\n",
    "# Get the number of the machine cores\n",
    "SIZE = COMM.Get_size()\n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "    if RANK == 0:\n",
    "        \n",
    "        #Only the controller process (rank 0) reads the input\n",
    "        value = int(input(\"Enter an integer value (or a negative integer to exit):\"))\n",
    "        \n",
    "        COMM.send( value, dest= 1)\n",
    "        \n",
    "    \n",
    "    else :\n",
    "        \n",
    "        value = COMM.recv( source= RANK-1)\n",
    "        \n",
    "        if RANK < SIZE-1 :\n",
    "            COMM.send( value + RANK, dest= RANK + 1)\n",
    "        \n",
    "        \n",
    "    if value < 0 :\n",
    "        \n",
    "        break\n",
    "        \n",
    "   \n",
    "        \n",
    "    # Print the received value and the rank of the current process\n",
    "    \n",
    "    print(f\"Process {RANK} got {value}\")\n",
    "    COMM.Barrier()\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66708ef6",
   "metadata": {},
   "source": [
    "# Exercice 4 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0dee1de",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MPI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m comm \u001b[38;5;241m=\u001b[39m \u001b[43mMPI\u001b[49m\u001b[38;5;241m.\u001b[39mCOMM_WORLD\n\u001b[1;32m      3\u001b[0m rank \u001b[38;5;241m=\u001b[39m comm\u001b[38;5;241m.\u001b[39mGet_rank()\n\u001b[1;32m      4\u001b[0m size \u001b[38;5;241m=\u001b[39m comm\u001b[38;5;241m.\u001b[39mGet_size()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MPI' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "# define matrix dimensions\n",
    "n = 8\n",
    "m = 8\n",
    "# create the matrix on process 0\n",
    "if rank == 0:\n",
    "    matrix = np.arange(n*m, dtype='float64').reshape(n, m)\n",
    "    print(\"Original matrix on processor 0:\")\n",
    "    print(matrix)\n",
    "else:\n",
    "    matrix = None\n",
    "# divide the matrix into parts and scatter to the other processes\n",
    "sendcounts = [n//2 * m//2, n//2 * m//2, n//2 * m//2]\n",
    "displs = [0, n//2 * m//2, 3*n//4 * m//2]\n",
    "submatrix = np.empty((n//2, m//2), dtype='float64')\n",
    "comm.Scatterv([matrix, sendcounts, displs, MPI.DOUBLE], submatrix, root=0)\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "if rank == 0:\n",
    "    # Create a 4x4 matrix\n",
    "    matrix = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])\n",
    "    print(\"Original matrix on processor 0:\")\n",
    "    print(matrix)\n",
    "    # Define how many rows each processor should receive\n",
    "    rows_per_proc = [2, 1, 1]\n",
    "    # Define the displacements for each processor\n",
    "    displacements = [0, 8, 12]\n",
    "else:\n",
    "    # Initialize empty matrix\n",
    "    matrix = None\n",
    "# Define rows_per_proc and displacements for non-zero processors\n",
    "    rows_per_proc = None\n",
    "    displacements = None\n",
    "# Scatter parts of the matrix to different processors\n",
    "d1_local = np.zeros(3)\n",
    "rows = comm.Scatterv([matrix, rows_per_proc, displacements, MPI.INT],d1_local, root=0)\n",
    "print(f\"Received matrix on processor {rank}:\")\n",
    "print(rows)\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "n = 8\n",
    "m = 8\n",
    "if rank == 0:\n",
    "    n = 8\n",
    "    m = 8\n",
    "    A = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            A[i, j] = i * m + j + 1\n",
    "    print(\"Original matrix on processor 0:\")\n",
    "    print(A)\n",
    "# Divide the matrix into parts to send to each processor\n",
    "    sendcounts = np.zeros(size, dtype=int)\n",
    "    displs = np.zeros(size, dtype=int)\n",
    "    sendcounts[1] = (n // 2) * (m - m // 2)\n",
    "    sendcounts[2] = (n - n // 2) * (m // 2)\n",
    "    sendcounts[3] = (n - n // 2) * (m - m // 2)\n",
    "    displs[1] = (n // 2)\n",
    "    displs[2] = (m //2) \n",
    "    displs[3] = (n - n // 2) * (m - m // 2)\n",
    "else:\n",
    "    A = None\n",
    "    sendcounts = None\n",
    "    displs = None\n",
    "# Scatter the matrix parts to each processor\n",
    "recvA = np.zeros((n // 2, m // 2))\n",
    "recvcounts = (n // 2) * (m // 2)\n",
    "print(np.transpose(A))\n",
    "comm.Scatterv([np.transpose(A) , sendcounts, displs, MPI.DOUBLE], recvA, root=0)\n",
    "if rank == 1:\n",
    "    print(\"Received matrix on processor 1:\")\n",
    "    print(recvA)\n",
    "elif rank == 2:\n",
    "    print(\"Received matrix on processor 2:\")\n",
    "    print(recvA)\n",
    "elif rank == 3:\n",
    "    print(\"Received matrix on processor 2:\")\n",
    "    print(recvA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5dff44",
   "metadata": {},
   "source": [
    "# Exercice 5 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e064d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.sparse import lil_matrix\n",
    "from numpy.random import rand, seed\n",
    "from numba import njit\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "''' This program compute parallel csc matrix vector multiplication using mpi '''\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "size = COMM.Get_size()\n",
    "RANK = COMM.Get_rank()\n",
    "\n",
    "seed(42)\n",
    "\n",
    "def matrixVectorMult(A, b, x, size):\n",
    "    \n",
    "    row, col = A.shape\n",
    "    start = RANK * (row // size)\n",
    "    end = start + (row // size)\n",
    "   #x = np.zeros(col)\n",
    "    for i in range(start, end):\n",
    "       # a = A[i]\n",
    "        for j in range(col):\n",
    "            x[i] += A[i,j] * b[j]\n",
    "\n",
    "    return x\n",
    "\n",
    "########################initialize matrix A and vector b ######################\n",
    "#matrix sizes\n",
    "SIZE = 1000\n",
    "#Local_size = \n",
    "\n",
    "# counts = block of each proc\n",
    "#counts = \n",
    "\n",
    "if RANK == 0:\n",
    "    A = lil_matrix((SIZE, SIZE))\n",
    "    A[0, :100] = rand(100)\n",
    "    A[1, 100:200] = A[0, :100]\n",
    "    A.setdiag(rand(SIZE))\n",
    "    A = A.toarray()\n",
    "    b = rand(SIZE)\n",
    "else :\n",
    "    A = None\n",
    "    b = None\n",
    "\n",
    "\n",
    "#########Send b to all procs and scatter A (each proc has its own local matrix#####\n",
    "LocalMatrix = lil_matrix((SIZE, SIZE))\n",
    "# Scatter the matrix A\n",
    "#COMM.Scatter(A, LocalMatrix, root=0)\n",
    "#####################Compute A*b locally#######################################\n",
    "b = np.zeros(SIZE)\n",
    "Localb = np.zeros(SIZE) \n",
    "#LocalX = np.zeros(SIZE)\n",
    "#COMM.Scatter(b, Localb, root=0)\n",
    "\n",
    "LocalX = np.zeros(SIZE)\n",
    "LocalX = matrixVectorMult(LocalMatrix, b, LocalX, size)\n",
    "if RANK == 0:\n",
    "    X = np.zeros(SIZE)\n",
    "\n",
    "#recvcounts = np.full(4, SIZE // 4)\n",
    "#displs = np.arange(4) * (SIZE // 4)\n",
    "#COMM.Gatherv(LocalX, [X, recvcounts, displs, MPI.DOUBLE], root = 0)\n",
    "# start = MPI.Wtime()\n",
    "# matrixVectorMult(LocalMatrix, b, LocalX)\n",
    "# stop = MPI.Wtime()\n",
    "\n",
    "if RANK == 0:\n",
    "    start = MPI.Wtime()\n",
    "    matrixVectorMult(LocalMatrix, b, LocalX, size)\n",
    "    stop = MPI.Wtime()\n",
    "    print(\"CPU time of parallel multiplication is \", (stop - start))\n",
    "\n",
    "##################Gather te results ###########################################\n",
    "# sendcouns = local size of result\n",
    "#sendcounts = \n",
    "if RANK == 0:\n",
    "    X = matrixVectorMult(A, b, LocalX, size) \n",
    "else :\n",
    "    X = None\n",
    "\n",
    "# Gather the result into X\n",
    "\n",
    "\n",
    "##################Print the results ###########################################\n",
    "\n",
    "if RANK == 0 :\n",
    "    start = MPI.Wtime()\n",
    "    X_ = A.dot(b)\n",
    "    stop = MPI.Wtime()\n",
    "    print(\"The time to calculate A*b using dot is :\", stop - start)\n",
    "    # print(\"The result of A*b using parallel version is :\", X)\n",
    "    \n",
    "    \n",
    "\n",
    "if RANK == 0:\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(X)\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Parallel Matrix-Vector Multiplication')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682f0d07",
   "metadata": {},
   "source": [
    "# Exercice 6 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0764c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from mpi4py import MPI\n",
    "import math\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "N = 840\n",
    "\n",
    "pi_part = 0.0\n",
    "for i in range(rank, N, size):\n",
    "    pi_part += 1.0/(1.0 + ((i + 0.5)/N)**2)\n",
    "pi_part *= 4.0/N\n",
    "#Print the computed value of pi for each process\n",
    "print(\"Process\", rank, \"partial sum:\", pi_part)\n",
    "if rank == 0:\n",
    "    pi = pi_part\n",
    "    for i in range(1, size):\n",
    "        pi += comm.recv(source=i)\n",
    "else:\n",
    "    comm.send(pi_part, dest=0)\n",
    "if rank == 0:\n",
    "    print(\"Computed pi:\", pi)\n",
    "comm.Barrier() \n",
    "t1 = MPI.Wtime()\n",
    "num_runs = 10000 \n",
    "for i in range(num_runs):\n",
    "    pi_part = 0.0\n",
    "    for j in range(rank, N, size):\n",
    "        #Divide the iterations among the processes\n",
    "        pi_part += 1.0/(1.0 + ((j + 0.5)/N)**2)\n",
    "    pi_part *= 4.0/N\n",
    "    #Collect and sum the partial results on the rank 0 process\n",
    "    if rank == 0:\n",
    "        pi = pi_part\n",
    "        for j in range(1, size):\n",
    "            pi += comm.recv(source=j)\n",
    "    else:\n",
    "        comm.send(pi_part, dest=0)\n",
    "    if rank == 0 and i == 0:\n",
    "        print(\"Computed pi:\", pi)\n",
    "comm.Barrier()\n",
    "t2 = MPI.Wtime()\n",
    "if rank == 0:\n",
    "    print(\"Taken Time:\", t2 - t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
