{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a7c1d19",
   "metadata": {},
   "source": [
    "# Exercice 1 :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3fa066",
   "metadata": {},
   "source": [
    "###  1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a4df9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hello_world_mpi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hello_world_mpi.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "RANK = COMM.Get_rank()\n",
    "SIZE = COMM.Get_size()\n",
    "\n",
    "print(\"Hello world !\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73428afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world !\r\n",
      "Hello world !\r\n",
      "Hello world !\r\n",
      "Hello world !\r\n"
     ]
    }
   ],
   "source": [
    "!mpirun -n 4 python3  hello_world_mpi.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3458ec6c",
   "metadata": {},
   "source": [
    "### 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "549ebbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hello_world_mpi_r_s.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hello_world_mpi_r_s.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "RANK = COMM.Get_rank()\n",
    "SIZE = COMM.Get_size()\n",
    "\n",
    "def h_w():\n",
    "    print(\"Hello world !\",RANK,SIZE)\n",
    "    \n",
    "h_w()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c2dc8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world !\r\n",
      "Hello world !\r\n",
      "Hello world !\r\n",
      "Hello world !\r\n",
      "Hello world !\r\n",
      "Hello world !\r\n",
      "Hello world !\r\n",
      "Hello world !\r\n"
     ]
    }
   ],
   "source": [
    "!mpirun --oversubscribe -n 8 python3  hello_world_mpi.py #to use more than the real cores existing on the machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d820d9d9",
   "metadata": {},
   "source": [
    "### 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b716a9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hello_world_mpi_r_s_0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hello_world_mpi_r_s_0.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "RANK = COMM.Get_rank()\n",
    "SIZE = COMM.Get_size()\n",
    "\n",
    "if RANK==0 :\n",
    "    print(\"Hello world !\",RANK,SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f8e8366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world !\r\n",
      "Hello world !\r\n",
      "Hello world !\r\n",
      "Hello world !\r\n"
     ]
    }
   ],
   "source": [
    "!mpirun -n 4 python3  hello_world_mpi.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8252f717",
   "metadata": {},
   "source": [
    "# Exercice 2 :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ba9b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile neg_value_exit.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "# Initialize MPI environment\n",
    "COMM = MPI.COMM_WORLD\n",
    "\n",
    "# Get the rank of the current process\n",
    "RANK= COMM.Get_rank()\n",
    "\n",
    "while True:\n",
    "    if RANK == 0:\n",
    "        # Only the controller process (rank 0) reads the input\n",
    "        value = int(input(\"Enter an integer value (or a negative integer to exit):\"))\n",
    "        \n",
    "    else:\n",
    "        value = None\n",
    "        \n",
    "    # Send the value to all processes\n",
    "    value = COMM.bcast(value, root=0)\n",
    "    \n",
    "    # Exit the program in case of negative input\n",
    "    if value < 0:\n",
    "        break\n",
    "\n",
    "    \n",
    "    # Print the received value and the rank of the current process\n",
    "    print(f\"Process {RANK} got {value}\")\n",
    "    COMM.Barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be64f639",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -n 4 python3  neg_value_exit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0199c75e",
   "metadata": {},
   "source": [
    "# Exercice 3 :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7655ecc",
   "metadata": {},
   "source": [
    "### 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7f133b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting broadcast_by_ring.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile broadcast_by_ring.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "# Initialize MPI environment\n",
    "COMM = MPI.COMM_WORLD\n",
    "\n",
    "# Get the rank of the current process\n",
    "RANK= COMM.Get_rank()\n",
    "\n",
    "# Get the number of the machine cores\n",
    "SIZE = COMM.Get_size()\n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "    if RANK == 0:\n",
    "        \n",
    "        #Only the controller process (rank 0) reads the input\n",
    "        value = int(input(\"Enter an integer value (or a negative integer to exit):\"))\n",
    "        \n",
    "        COMM.send( value, dest= 1)\n",
    "        \n",
    "    \n",
    "    else :\n",
    "        \n",
    "        value = COMM.recv( source= RANK-1)\n",
    "        \n",
    "        if RANK < SIZE-1 :\n",
    "            COMM.send( value + RANK, dest= RANK + 1)\n",
    "        \n",
    "        \n",
    "    if value < 0 :\n",
    "        \n",
    "        break\n",
    "        \n",
    "   \n",
    "        \n",
    "    # Print the received value and the rank of the current process\n",
    "    \n",
    "    print(f\"Process {RANK} got {value}\")\n",
    "    COMM.Barrier()\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66708ef6",
   "metadata": {},
   "source": [
    "# Exercice 4 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "322ef7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      " [[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]\n",
      " [16 17 18 19]\n",
      " [20 21 22 23]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'int' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Define scatterv parameters\u001b[39;00m\n\u001b[1;32m     21\u001b[0m counts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(n\u001b[38;5;241m*\u001b[39mm\u001b[38;5;241m/\u001b[39msize)] \u001b[38;5;241m*\u001b[39m size\n\u001b[0;32m---> 22\u001b[0m counts[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mn\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     23\u001b[0m displs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(counts[:i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(size)]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Scatter parts of matrix A to other processors\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'int' and 'list'"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# Define matrix size\n",
    "n = 6\n",
    "m = 4\n",
    "\n",
    "# Create matrix A on processor 0\n",
    "if rank == 0:\n",
    "    A = np.arange(n*m).reshape((n, m))\n",
    "    print(\"Matrix A:\\n\", A)\n",
    "else:\n",
    "    A = None\n",
    "\n",
    "# Define scatterv parameters\n",
    "counts = [int(n*m/size)] * size\n",
    "counts[0] = n*m - counts[1:]\n",
    "displs = [sum(counts[:i]) for i in range(size)]\n",
    "\n",
    "# Scatter parts of matrix A to other processors\n",
    "recvbuf = np.zeros(int(n*m/size), dtype='i')\n",
    "comm.Scatterv([A, counts, displs, MPI.INT], recvbuf, root=0)\n",
    "\n",
    "# Processor 1 receives A(i,j) for i=0 to (n/2)-1 and j=m/2 to m-1\n",
    "if rank == 1:\n",
    "    recvbuf = recvbuf.reshape((int(n/2), int(m/2)))\n",
    "    print(\"Processor\", rank, \"received:\\n\", recvbuf)\n",
    "\n",
    "# Processor 2 receives A(i,j) for i=n/2 to n-1 and j=0 to (m/2)-1\n",
    "if rank == 2:\n",
    "    recvbuf = recvbuf.reshape((int(n/2), int(m/2)))\n",
    "    print(\"Processor\", rank, \"received:\\n\", recvbuf)\n",
    "\n",
    "# Processor 3 receives A(i,j) for i=n/2 to n-1 and j=m/2 to m-1\n",
    "if rank == 3:\n",
    "    recvbuf = recvbuf.reshape((int(n/2), int(m/2)))\n",
    "    print(\"Processor\", rank, \"received:\\n\", recvbuf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5dff44",
   "metadata": {},
   "source": [
    "# Exercice 5 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e064d878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time of parallel multiplication using 1 processes is 0.001518\n",
      "The error comparing to the dot product is : 7747.438821026195\n"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Matrix-vector multiplication function\n",
    "def matrix_vector_mult(A, x):\n",
    "    m = A.shape[0]\n",
    "    n = A.shape[1]\n",
    "    y = np.zeros((m,))\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            y[i] += A[i,j]*x[j]\n",
    "    return y\n",
    "\n",
    "# MPI setup\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# Initialize matrix and vector\n",
    "m = 1000\n",
    "n = 1000\n",
    "A = np.random.rand(m, n)\n",
    "x = np.random.rand(n,)\n",
    "\n",
    "# Scatter the matrix and vector\n",
    "sendcounts = [m//size]*size\n",
    "sendcounts[-1] += m % size\n",
    "senddispls = [sum(sendcounts[:i]) for i in range(size)]\n",
    "local_A = np.zeros((sendcounts[rank], n), dtype=float)\n",
    "comm.Scatterv([A, sendcounts, senddispls, MPI.DOUBLE], local_A, root=0)\n",
    "\n",
    "sendcounts = [n//size]*size\n",
    "sendcounts[-1] += n % size\n",
    "senddispls = [sum(sendcounts[:i]) for i in range(size)]\n",
    "local_x = np.zeros(sendcounts[rank], dtype=float)\n",
    "comm.Scatterv([x, sendcounts, senddispls, MPI.DOUBLE], local_x, root=0)\n",
    "\n",
    "# Matrix-vector multiplication on each process\n",
    "local_y = matrix_vector_mult(local_A, local_x)\n",
    "\n",
    "# Gather the results\n",
    "recvcounts = [m//size]*size\n",
    "recvcounts[-1] += m % size\n",
    "recvdispls = [sum(recvcounts[:i]) for i in range(size)]\n",
    "y = np.zeros((m,), dtype=float)\n",
    "comm.Gatherv(local_y, [y, recvcounts, recvdispls, MPI.DOUBLE], root=0)\n",
    "\n",
    "# Compute the dot product on the root process\n",
    "if rank == 0:\n",
    "    start_time = time.time()\n",
    "    y_dot = np.dot(A, x)\n",
    "    dot_time = time.time() - start_time\n",
    "    \n",
    "    # Compare the results\n",
    "    error = np.linalg.norm(y-y_dot)\n",
    "    print(\"CPU time of parallel multiplication using {} processes is {:.6f}\".format(size, dot_time))\n",
    "    print(\"The error comparing to the dot product is :\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682f0d07",
   "metadata": {},
   "source": [
    "# Exercice 6 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0764c81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 0 partial sum: 3.1415927716925873\n",
      "Computed pi: 3.1415927716925873\n",
      "Computed pi: 3.1415927716925873\n",
      "Time taken: 3.181356204\n"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "import math\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "N = 840\n",
    "\n",
    "# Compute the value of pi independently on each process\n",
    "pi_part = 0.0\n",
    "for i in range(rank, N, size):\n",
    "    pi_part += 1.0/(1.0 + ((i + 0.5)/N)**2)\n",
    "\n",
    "pi_part *= 4.0/N\n",
    "\n",
    "# Print the partial sum computed by each process\n",
    "print(\"Process\", rank, \"partial sum:\", pi_part)\n",
    "\n",
    "# Accumulate the partial sums on the controller (rank 0)\n",
    "if rank == 0:\n",
    "    pi = pi_part\n",
    "    for i in range(1, size):\n",
    "        pi += comm.recv(source=i)\n",
    "else:\n",
    "    comm.send(pi_part, dest=0)\n",
    "\n",
    "# Print the final result computed by the controller\n",
    "if rank == 0:\n",
    "    print(\"Computed pi:\", pi)\n",
    "\n",
    "# Measure the time taken for the computation\n",
    "comm.Barrier() # synchronize all processes\n",
    "t1 = MPI.Wtime()\n",
    "\n",
    "num_runs = 10000 # number of times to compute pi\n",
    "for i in range(num_runs):\n",
    "    pi_part = 0.0\n",
    "    for j in range(rank, N, size):\n",
    "        pi_part += 1.0/(1.0 + ((j + 0.5)/N)**2)\n",
    "    pi_part *= 4.0/N\n",
    "\n",
    "    if rank == 0:\n",
    "        pi = pi_part\n",
    "        for j in range(1, size):\n",
    "            pi += comm.recv(source=j)\n",
    "    else:\n",
    "        comm.send(pi_part, dest=0)\n",
    "\n",
    "    if rank == 0 and i == 0:\n",
    "        print(\"Computed pi:\", pi)\n",
    "\n",
    "# Record the time taken\n",
    "comm.Barrier() # synchronize all processes\n",
    "t2 = MPI.Wtime()\n",
    "\n",
    "if rank == 0:\n",
    "    print(\"Time taken:\", t2 - t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
